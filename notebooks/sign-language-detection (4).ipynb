{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip install gdown -qq\n!pip install git+https://github.com/tensorflow/docs\n# !gdown --id 11eFE_quM2_2-h3H_zTTjq0i0D6pkx62Z\n# !unzip ../input/wlasl-processed -d ./WLASL2000\n# !gdown --id 13lZfwNZcTm6BVnUU0sqyrzwUL-dyaCos\n# https://drive.google.com/file/d/13lZfwNZcTm6BVnUU0sqyrzwUL-dyaCos/view?usp=sharing","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2022-02-06T08:44:24.573252Z","iopub.execute_input":"2022-02-06T08:44:24.573611Z","iopub.status.idle":"2022-02-06T08:44:46.309466Z","shell.execute_reply.started":"2022-02-06T08:44:24.573498Z","shell.execute_reply":"2022-02-06T08:44:46.308601Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import os\nimport re\nimport scipy\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport sklearn\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import LabelEncoder,OrdinalEncoder,OneHotEncoder\nimport shutil\nimport random\nfrom sklearn.metrics import log_loss\nimport matplotlib.pyplot as plt\nimport tensorflow as tf\nfrom tqdm import tqdm\nfrom keras import Sequential, Input, layers\nfrom keras.regularizers import l2\nfrom keras.layers import Dense, BatchNormalization, ReLU, Dropout, MaxPooling2D, Conv2D, Flatten, BatchNormalization, Dropout\nimport tensorflow_hub as hub\nfrom tensorflow.keras.optimizers import Adam\nimport cv2 as cv\n# TensorFlow and TF-Hub modules.\nfrom absl import logging\n\nimport tensorflow as tf\nimport tensorflow_hub as hub\nfrom tensorflow_docs.vis import embed\n\nlogging.set_verbosity(logging.ERROR)\n\n# Some modules to help with reading the UCF101 dataset.\nimport random\nimport re\nimport os\nimport tempfile\nimport ssl\nimport cv2\nimport numpy as np\n\n# Some modules to display an animation using imageio.\nimport imageio\nfrom IPython import display\n\nfrom urllib import request  # requires python3\npath = './'\nid3 = \"https://tfhub.dev/deepmind/i3d-kinetics-400/1\"","metadata":{"execution":{"iopub.status.busy":"2022-02-06T08:44:46.313004Z","iopub.execute_input":"2022-02-06T08:44:46.314765Z","iopub.status.idle":"2022-02-06T08:44:52.173703Z","shell.execute_reply.started":"2022-02-06T08:44:46.314729Z","shell.execute_reply":"2022-02-06T08:44:52.172482Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"path2 = \"../input/wlasl-complete/WLASL2000\"\nvideos = os.listdir(path2)\nprint(len(videos))\nwlas_df = pd.read_json(\"../input/wlasl-processed/WLASL_v0.3.json\")\nwlas_df.head()\nwlas_df['instances'][0][:2]\ndef get_features(feat) :\n    ids = []\n    urls = []\n    for f in feat :\n        id = f['video_id']\n        url = f['url']\n#         print(id)\n        if os.path.exists(f\"{path2}/{id}.mp4\") :\n            ids.append(id)\n            urls.append(url)\n    return ids,urls\n\ndef get_video_ids(feats) :\n    videos = []\n    for feat in feats :\n        video_id = feat['video_id']\n#         print(video_id)\n        if os.path.exists(f\"{path2}/{video_id}.mp4\") :\n            videos.append(video_id)\n    return videos\n\ndf = pd.DataFrame(columns=['gloss','video_id','url'])\nwlas_df['videos_ids'] = wlas_df['instances'].apply(get_video_ids)\nwlas_df\nfor row in tqdm(wlas_df.iterrows()) :\n    ids,urls = get_features(row[1][1])\n    words = [row[1][0]]*len(ids)\n    df = df.append(pd.DataFrame(list(zip(words,ids,urls)),\n                   columns=df.columns),ignore_index=True)\ndf.to_csv('feature.csv', index=False)\nwlas_df.to_csv('wlas_df.csv',index=False)\nprint(df.shape)\ndf.head()\n\ndef extend_str(s,count=5) :\n    s = str(s)\n    s = (count-len(s))*\"0\"+s\n    return s\n\ndf['video_id'] = df['video_id'].apply(extend_str)\n# frame_count = []\n# for video_id in tqdm(df['video_id']) :\n#     video = cv.VideoCapture(f\"{path2}/{video_id}.mp4\")\n#     length = int(video.get(cv.CAP_PROP_FRAME_COUNT))\n#     frame_count.append(length)\n# #     break\n\n# df['frame_count'] = frame_count\n# print(df['frame_count'].max(),df['frame_count'].min())\n# sns.histplot(df['frame_count'])\n# plt.show()\ndf.head()","metadata":{"execution":{"iopub.status.busy":"2022-02-06T08:44:52.180175Z","iopub.execute_input":"2022-02-06T08:44:52.182073Z","iopub.status.idle":"2022-02-06T08:45:28.310151Z","shell.execute_reply.started":"2022-02-06T08:44:52.182027Z","shell.execute_reply":"2022-02-06T08:45:28.309408Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.model_selection import StratifiedKFold\nskf = StratifiedKFold(n_splits=10)\ndf['type'] = 0\nfor i,(train_idx,val_idx) in enumerate(skf.split(df,df['gloss'])) :\n    if i == 0 :\n        df.loc[train_idx,'type'] = 'train'\n        df.loc[val_idx,'type'] = 'val' \n    elif i == 1 :\n        df.loc[val_idx,'type'] = 'test'\n    else :\n        break\n        \npaths_arr=['train','test','val']\nfor i in paths_arr :\n    if not os.path.exists(f\"{path}/{i}\") :\n        os.makedirs(f'{path}/{i}')\n\nobs = df['gloss'].unique().tolist()\nfor i in obs :\n    for j in paths_arr :\n        if not os.path.exists(f\"{path}/{j}/{i}\") :\n            os.makedirs(f'{path}/{j}/{i}')\n\nfor row in tqdm(df.iterrows()) :\n    type_type = row[1].type\n    vid_id = row[1].video_id\n    gloss = row[1].gloss\n    shutil.copy(f\"{path2}/{vid_id}.mp4\",f\"{path}/{type_type}/{gloss}/{vid_id}.mp4\")\n# !zip -r train.zip \"./train\" \n# !zip -r test.zip \"./test\" \n# !zip -r val.zip \"./val\" ","metadata":{"execution":{"iopub.status.busy":"2022-02-06T08:45:28.312375Z","iopub.execute_input":"2022-02-06T08:45:28.312822Z","iopub.status.idle":"2022-02-06T08:47:09.215284Z","shell.execute_reply.started":"2022-02-06T08:45:28.31278Z","shell.execute_reply":"2022-02-06T08:47:09.214516Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<a href=\"./train.zip\"> Download train File </a>\n<a href=\"./test.zip\"> Download test File </a>\n<a href=\"./val.zip\"> Download val File </a>","metadata":{}},{"cell_type":"code","source":"def crop_center_square(frame):\n    y, x = frame.shape[0:2]\n    min_dim = min(y, x)\n    start_x = (x // 2) - (min_dim // 2)\n    start_y = (y // 2) - (min_dim // 2)\n    return frame[start_y:start_y+min_dim,start_x:start_x+min_dim]\n\ndef load_video(path, max_frames=0, resize=(224, 224)):\n    cap = cv2.VideoCapture(path)\n    frames = []\n    try:\n        while True:\n            ret, frame = cap.read()\n            if not ret:\n                break\n            frame = crop_center_square(frame)\n            frame = cv2.resize(frame, resize)\n            frame = frame[:, :, [2, 1, 0]]\n            frames.append(frame)\n\n            if len(frames) == max_frames:\n                break\n    finally:\n        cap.release()\n    return np.array(frames) / 255.0\n\ndef to_gif(images):\n    converted_images = np.clip(images * 255, 0, 255).astype(np.uint8)\n    imageio.mimsave('./animation.gif', converted_images, fps=25)\n    return embed.embed_file('./animation.gif')\n\n\ni3d = hub.load(\"https://tfhub.dev/deepmind/i3d-kinetics-400/1\").signatures['default']\ni3d\nsample_video = load_video(f\"{path}/{df.loc[0,'type']}/{df.loc[0,'gloss']}/{df.loc[0,'video_id']}.mp4\",64)\nsample_video.shape","metadata":{"execution":{"iopub.status.busy":"2022-02-06T08:47:09.216742Z","iopub.execute_input":"2022-02-06T08:47:09.217289Z","iopub.status.idle":"2022-02-06T08:47:13.763316Z","shell.execute_reply.started":"2022-02-06T08:47:09.217245Z","shell.execute_reply":"2022-02-06T08:47:13.762468Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def predict(sample_video):\n  # Add a batch axis to the sample video.\n    model_input = tf.constant(sample_video, dtype=tf.float32)[tf.newaxis, ...]\n\n    logits = i3d(model_input)['default'][0]\n    probabilities = tf.nn.softmax(logits)\n\n    print(\"Top 5 actions:\")\n    for i in np.argsort(probabilities)[::-1][:5]:\n        print(f\"  {obs[i]}: {probabilities[i] * 100:5.2f}%\")\npredict(sample_video)\nto_gif(sample_video)","metadata":{"execution":{"iopub.status.busy":"2022-02-06T08:47:13.764823Z","iopub.execute_input":"2022-02-06T08:47:13.765095Z","iopub.status.idle":"2022-02-06T08:47:22.076912Z","shell.execute_reply.started":"2022-02-06T08:47:13.765057Z","shell.execute_reply":"2022-02-06T08:47:22.074612Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.preprocessing import LabelEncoder\nle = LabelEncoder()\nle.fit(obs)\nle.transform(obs)","metadata":{"execution":{"iopub.status.busy":"2022-02-06T08:47:22.077913Z","iopub.execute_input":"2022-02-06T08:47:22.07816Z","iopub.status.idle":"2022-02-06T08:47:22.091839Z","shell.execute_reply.started":"2022-02-06T08:47:22.078127Z","shell.execute_reply":"2022-02-06T08:47:22.090979Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# feature_extractor_model = \"https://tfhub.dev/deepmind/i3d-kinetics-400/1\"\n# BATCH_SIZE=32\n# NO_FRAMES=50\n# IMG_WIDTH=224\n# IMG_HEIGHT=224\n# CHANNELS=3\n# n_classes = len(obs)\n# input_shape = (NO_FRAMES,IMG_WIDTH,IMG_HEIGHT,CHANNELS)\n# feature_extractor_layer = hub.KerasLayer(\n#         feature_extractor_model,\n#         input_shape=input_shape,\n#         trainable=False)\n\n\n# def crop_center_square(frame):\n#     y, x = frame.shape[0:2]\n#     min_dim = min(y, x)\n#     start_x = (x // 2) - (min_dim // 2)\n#     start_y = (y // 2) - (min_dim // 2)\n#     return frame[start_y:start_y+min_dim,start_x:start_x+min_dim]\n\n# def load_video(path, max_frames=NO_FRAMES, resize=(IMG_WIDTH, IMG_HEIGHT)):\n#     print(path,max_frames)\n#     cap = cv2.VideoCapture(path)\n#     frames = []\n#     try:\n#         while True:\n#             ret, frame = cap.read()\n#             if not ret:\n#                 break\n#             frame = crop_center_square(frame)\n#             frame = cv2.resize(frame, resize)\n#             frame = frame[:, :, [2, 1, 0]]\n#             frames.append(frame)\n\n#             if len(frames) == max_frames:\n#                 break\n#     finally:\n#         cap.release()\n#     return np.array(frames) / 255.0\n\n# def to_gif(images):\n#     converted_images = np.clip(images * 255, 0, 255).astype(np.uint8)\n#     imageio.mimsave('./animation.gif', converted_images, fps=25)\n#     return embed.embed_file('./animation.gif')\n\n# model = Sequential([\n# #         Input((10, 224, 224, 3,)),\n#         feature_extractor_layer,\n#         Dense(n_classes, activation='softmax')\n#     ])\n# model.summary()\n\n\n# ds_train = df[df['type']=='train'][['gloss','video_id']]\n# ds_train['video_id'] = ds_train['video_id'].apply(lambda x:float(x))\n# ds_test = df[df['type']=='test'][['gloss','video_id']]\n# ds_test['video_id'] = ds_test['video_id'].apply(lambda x:float(x))\n# ds_val = df[df['type']=='val'][['gloss','video_id']]\n# ds_val['video_id'] = ds_val['video_id'].apply(lambda x:float(x))\n# ds_train = tf.data.Dataset.from_tensor_slices((ds_train['video_id'].values, le.transform(ds_train['gloss'].values)))\n# ds_test = tf.data.Dataset.from_tensor_slices((ds_test['video_id'].values, le.transform(ds_test['gloss'].values)))\n# ds_val = tf.data.Dataset.from_tensor_slices((ds_val['video_id'].values, le.transform(ds_val['gloss'].values)))\n# # ds_train = ds_train.map(load_video).batch(2)\n# print(ds_train)\n# for i in ds_train:\n#     print(i)\n#     break\n    \n# def load_video(path1,y, max_frames=NO_FRAMES, resize=(IMG_WIDTH, IMG_HEIGHT)):\n# #     print(path1)\n#     path = f\"../input/wlasl-complete/WLASL2000/{extend_str(str(int(path1.numpy())))}.mp4\"\n#     cap = cv2.VideoCapture(path)\n#     frames = np.zeros((max_frames,IMG_WIDTH,IMG_HEIGHT,CHANNELS))\n#     i = 0\n#     try:\n#         while True:\n#             ret, frame = cap.read()\n            \n#             if not ret:\n#                 break\n#             frame = crop_center_square(frame)\n#             frame = cv2.resize(frame, resize)\n#             frame = frame[:, :, [2, 1, 0]]\n#             frames[i] = frame\n#             i+=1\n\n#             if i == max_frames:\n#                 break\n#     finally:\n#         cap.release()\n# #     print(len(frames))\n#     return tf.constant(frames)/255.0,y\n\n# def process_video(x,y) :\n#     x,y=tf.py_function(load_video, [x,y], [tf.float64, tf.int64,])\n#     return x,y\n    \n    \n\n# # ds_train = ds_train.map(lambda x,y: tf.py_function(load_video, [x,y], [tf.float64, tf.int64,])).batch(BATCH_SIZE)\n# ds_train = ds_train.map(process_video).batch(BATCH_SIZE)\n# # ds_test = ds_test.map(lambda x,y: tf.py_function(load_video, [x,y], [tf.float64, tf.int64,])).batch(BATCH_SIZE)\n# ds_test = ds_test.map(process_video).batch(BATCH_SIZE)\n# # ds_val = ds_val.map(lambda x,y: tf.py_function(load_video, [x,y], [tf.float64, tf.int64,])).batch(BATCH_SIZE)\n# ds_val = ds_val.map(process_video).batch(BATCH_SIZE)\n# ds_train","metadata":{"execution":{"iopub.status.busy":"2022-02-06T08:47:22.093346Z","iopub.execute_input":"2022-02-06T08:47:22.093792Z","iopub.status.idle":"2022-02-06T08:47:22.10217Z","shell.execute_reply.started":"2022-02-06T08:47:22.093757Z","shell.execute_reply":"2022-02-06T08:47:22.101589Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"feature_extractor_model = \"https://tfhub.dev/deepmind/i3d-kinetics-400/1\"\nBATCH_SIZE=32\nSHUFFLE_WINDOW=32\nNO_FRAMES=50\nIMG_WIDTH=224\nIMG_HEIGHT=224\nCHANNELS=3\nn_classes = len(obs)\ninput_shape = (NO_FRAMES,IMG_WIDTH,IMG_HEIGHT,CHANNELS)\nfeature_extractor_layer = hub.KerasLayer(\n        feature_extractor_model,\n        input_shape=input_shape,\n        trainable=False)\n\n\ndef crop_center_square(frame):\n    y, x = frame.shape[0:2]\n    min_dim = min(y, x)\n    start_x = (x // 2) - (min_dim // 2)\n    start_y = (y // 2) - (min_dim // 2)\n    return frame[start_y:start_y+min_dim,start_x:start_x+min_dim]\n\ndef load_video(path, max_frames=NO_FRAMES, resize=(IMG_WIDTH, IMG_HEIGHT)):\n    print(path,max_frames)\n    cap = cv2.VideoCapture(path)\n    frames = []\n    try:\n        while True:\n            ret, frame = cap.read()\n            if not ret:\n                break\n            frame = crop_center_square(frame)\n            frame = cv2.resize(frame, resize)\n            frame = frame[:, :, [2, 1, 0]]\n            frames.append(frame)\n\n            if len(frames) == max_frames:\n                break\n    finally:\n        cap.release()\n    return np.array(frames) / 255.0\n\ndef to_gif(images):\n    converted_images = np.clip(images * 255, 0, 255).astype(np.uint8)\n    imageio.mimsave('./animation.gif', converted_images, fps=25)\n    return embed.embed_file('./animation.gif')\n\nmodel = Sequential([\n#         Input((10, 224, 224, 3,)),\n        feature_extractor_layer,\n        Dense(n_classes, activation='softmax')\n    ])\nmodel.summary()\n\nfrom sklearn.utils import shuffle\ndf = shuffle(df,random_state=42)\nds_train = df[df['type']=='train'][['gloss','video_id']]\nds_train['video_id'] = ds_train['video_id'].apply(lambda x:float(x))\nds_test = df[df['type']=='test'][['gloss','video_id']]\nds_test['video_id'] = ds_test['video_id'].apply(lambda x:float(x))\nds_val = df[df['type']=='val'][['gloss','video_id']]\nds_val['video_id'] = ds_val['video_id'].apply(lambda x:float(x))\nds_x_train = tf.constant(ds_train['video_id'].values)\nds_y_train = tf.constant(le.transform(ds_train['gloss'].values))\n# ds_y_train = tf.constant(ds_train['gloss'].values)\n\nds_x_test = tf.constant(ds_test['video_id'].values)\nds_y_test = tf.constant(le.transform(ds_test['gloss'].values))\n# ds_y_test = tf.constant(ds_test['gloss'].values)\n\nds_x_val = tf.constant(ds_val['video_id'].values)\nds_y_val = tf.constant(le.transform(ds_val['gloss'].values))\n# ds_y_val = tf.constant(ds_val['gloss'].values)\n\n# ds_train = ds_train.map(load_video).batch(2)\n\n    \ndef load_video(path1,y, max_frames=NO_FRAMES, resize=(IMG_WIDTH, IMG_HEIGHT)):\n#     print(path1)\n#     path = f\"../input/wlasl-complete/WLASL2000/{extend_str(str(int(path1.numpy())))}.mp4\"\n    path = f\"../input/wlasl-complete/WLASL2000/{extend_str(str(int(path1)))}.mp4\"\n    cap = cv2.VideoCapture(path)\n    frames = np.zeros((max_frames,IMG_WIDTH,IMG_HEIGHT,CHANNELS))\n    i = 0\n    try:\n        while True:\n            ret, frame = cap.read()\n            \n            if not ret:\n                break\n            frame = crop_center_square(frame)\n            frame = cv2.resize(frame, resize)\n            frame = frame[:, :, [2, 1, 0]]\n            frames[i] = frame\n            i+=1\n\n            if i == max_frames:\n                break\n    finally:\n        cap.release()\n#     print(len(frames))\n    return tf.constant(frames)/255.0,y\n\n\ndef gen(x,y):\n    for i,j in enumerate(x) :\n#         print(i,j)\n        x1,y1 = load_video(j,y[i])\n        yield x1,y1\n\nds_train = tf.data.Dataset.from_generator(\n    gen,\n    output_signature=(\n        tf.TensorSpec(shape=(NO_FRAMES,IMG_WIDTH,IMG_HEIGHT,CHANNELS), dtype=tf.float64),\n        tf.TensorSpec(shape=(), dtype=tf.int64)),\n    args=(ds_x_train, ds_y_train)\n).shuffle(SHUFFLE_WINDOW).batch(BATCH_SIZE)\n\nds_test = tf.data.Dataset.from_generator(\n    gen,\n    output_signature=(\n        tf.TensorSpec(shape=(NO_FRAMES,IMG_WIDTH,IMG_HEIGHT,CHANNELS), dtype=tf.float64),\n        tf.TensorSpec(shape=(), dtype=tf.int64)),\n    args=(ds_x_test, ds_y_test)\n).shuffle(SHUFFLE_WINDOW).batch(BATCH_SIZE)\n\nds_val = tf.data.Dataset.from_generator(\n    gen,\n    output_signature=(\n        tf.TensorSpec(shape=(NO_FRAMES,IMG_WIDTH,IMG_HEIGHT,CHANNELS), dtype=tf.float64),\n        tf.TensorSpec(shape=(), dtype=tf.int64)),\n    args=(ds_x_val, ds_y_val)\n).shuffle(SHUFFLE_WINDOW).batch(BATCH_SIZE)\nds_train","metadata":{"execution":{"iopub.status.busy":"2022-02-06T08:47:22.103769Z","iopub.execute_input":"2022-02-06T08:47:22.104263Z","iopub.status.idle":"2022-02-06T08:47:24.964618Z","shell.execute_reply.started":"2022-02-06T08:47:22.104225Z","shell.execute_reply":"2022-02-06T08:47:24.963845Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(ds_train.element_spec)\nfor image, label in ds_train.take(1):\n    print(\"Image shape: \", image.shape)\n    print(\"Label: \", label.numpy())\nprint(\"Number of samples in train: \", ds_train.cardinality().numpy(),\" in test: \",ds_test.cardinality().numpy())\ntf.data.experimental.UNKNOWN_CARDINALITY","metadata":{"execution":{"iopub.status.busy":"2022-02-06T08:47:24.966957Z","iopub.execute_input":"2022-02-06T08:47:24.967401Z","iopub.status.idle":"2022-02-06T08:47:37.579867Z","shell.execute_reply.started":"2022-02-06T08:47:24.967359Z","shell.execute_reply":"2022-02-06T08:47:37.579052Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"TF_NUM_EPOCHS=2\nmodel.compile(optimizer=Adam(learning_rate=3e-4),\n                loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n\nhistory = model.fit(ds_train,\n                    validation_data=ds_val,\n                    epochs=TF_NUM_EPOCHS, batch_size=BATCH_SIZE, verbose=1)","metadata":{"execution":{"iopub.status.busy":"2022-02-06T08:47:37.583778Z","iopub.execute_input":"2022-02-06T08:47:37.585727Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}